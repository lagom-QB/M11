{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lagom-QB/M11/blob/master/EMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UinBL84o1sml",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# EMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54606hJC0QEH",
        "colab_type": "text"
      },
      "source": [
        "EMNIST is an extention of MNIST dataset. It has 47 classes (handwritten digits and leters), some of the letters are represented as two classes (upper and lower case) and some (ex. o, s) has only one class associated with them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN6Uk4bp7bUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "train_dataset = datasets.EMNIST('/data', train=True, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]),\n",
        "                                split=\"balanced\")\n",
        "\n",
        "test_dataset = datasets.EMNIST('../data', train=False, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]),\n",
        "                                split=\"balanced\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXSDr7GIz_H1",
        "colab_type": "text"
      },
      "source": [
        "#1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzbIhZXw0ECa",
        "colab_type": "text"
      },
      "source": [
        "Find correspondence between classess and letters/digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLwX1Ah4OYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "letters = train_dataset.classes_split_dict['letters']\n",
        "digits  = train_dataset.classes_split_dict['digits']\n",
        "byclass = train_dataset.classes_split_dict['byclass']  # if not why not\n",
        "balanced = train_dataset.classes_split_dict['balanced']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXRQ_ta89Pv4",
        "colab_type": "code",
        "outputId": "14b44e2e-ab2f-48e8-f219-b7d19f622cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(balanced)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10X16_Sz5gtP",
        "colab_type": "text"
      },
      "source": [
        "#2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE8iDFp05y0f",
        "colab_type": "text"
      },
      "source": [
        "Build and train a dense classifier for EMNIST dataset. Try to achieve the highest accuracy you can.\\\n",
        "Explain, what experiments have you perform.\\\n",
        "What optimizers have you tested?\\\n",
        "How network/optimizer parameters were chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpjCl-Ge6CXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.modules import loss\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTsGgOG28kaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001 # Small enough\n",
        "num_epochs = 25 #If not why not\n",
        "num_classes = len(balanced)\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdTfLAX8XNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "\n",
        "device    = torch.device(\"cuda\")\n",
        "# device    = torch.device(\"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_1 = torch.optim.Adam(\n",
        "                    model.parameters(), \n",
        "                    lr=learning_rate)  \n",
        "\n",
        "optimizer = torch.optim.Adagrad(\n",
        "                    model.parameters(),\n",
        "                    lr = learning_rate, \n",
        "                    lr_decay = 0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "                                            optimizer,\n",
        "                                            0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fey_rbnsYvMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_ass(writer, model, test_loader, loss_function, device = device):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "    \n",
        "            output = model(data)\n",
        "    \n",
        "            test_loss += loss_function(output, target).sum().item()\n",
        "    \n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "    \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    writer.add_scalar(\"test_loss\", test_loss, global_step=epoch)\n",
        "    writer.add_scalar(\"accuracy\", accuracy, global_step=epoch)\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbqw2Emv9aq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ass(writer, model, train_loader, optimizer, loss_function, epoch , device = device):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    writer.add_scalar(\"train_loss\", train_loss, global_step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkm5eekaQliD",
        "colab_type": "code",
        "outputId": "a8a334a9-6e7e-4e97-933b-f567fdf3e496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "writer_2 = SummaryWriter(log_dir=f\"log_2\")\n",
        "for epoch in range(20):\n",
        "    train_ass(writer_2, model, train_loader, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(writer_2, model, test_loader, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/112800 (0%)]\tLoss: 4.151545\n",
            "Train Epoch: 0 [32000/112800 (28%)]\tLoss: 1.078117\n",
            "Train Epoch: 0 [64000/112800 (57%)]\tLoss: 1.097522\n",
            "Train Epoch: 0 [96000/112800 (85%)]\tLoss: 1.090675\n",
            "\n",
            "Test set: Average loss: 0.0161, Accuracy: 13231/18800 (70%)\n",
            "\n",
            "Train Epoch: 1 [0/112800 (0%)]\tLoss: 0.906145\n",
            "Train Epoch: 1 [32000/112800 (28%)]\tLoss: 0.884269\n",
            "Train Epoch: 1 [64000/112800 (57%)]\tLoss: 1.054964\n",
            "Train Epoch: 1 [96000/112800 (85%)]\tLoss: 1.046314\n",
            "\n",
            "Test set: Average loss: 0.0158, Accuracy: 13336/18800 (71%)\n",
            "\n",
            "Train Epoch: 2 [0/112800 (0%)]\tLoss: 0.689648\n",
            "Train Epoch: 2 [32000/112800 (28%)]\tLoss: 1.214744\n",
            "Train Epoch: 2 [64000/112800 (57%)]\tLoss: 0.943665\n",
            "Train Epoch: 2 [96000/112800 (85%)]\tLoss: 1.169741\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 13376/18800 (71%)\n",
            "\n",
            "Train Epoch: 3 [0/112800 (0%)]\tLoss: 1.224070\n",
            "Train Epoch: 3 [32000/112800 (28%)]\tLoss: 0.968024\n",
            "Train Epoch: 3 [64000/112800 (57%)]\tLoss: 1.333064\n",
            "Train Epoch: 3 [96000/112800 (85%)]\tLoss: 0.830782\n",
            "\n",
            "Test set: Average loss: 0.0155, Accuracy: 13405/18800 (71%)\n",
            "\n",
            "Train Epoch: 4 [0/112800 (0%)]\tLoss: 1.134893\n",
            "Train Epoch: 4 [32000/112800 (28%)]\tLoss: 0.850409\n",
            "Train Epoch: 4 [64000/112800 (57%)]\tLoss: 0.927137\n",
            "Train Epoch: 4 [96000/112800 (85%)]\tLoss: 1.086798\n",
            "\n",
            "Test set: Average loss: 0.0155, Accuracy: 13415/18800 (71%)\n",
            "\n",
            "Train Epoch: 5 [0/112800 (0%)]\tLoss: 0.783714\n",
            "Train Epoch: 5 [32000/112800 (28%)]\tLoss: 1.297207\n",
            "Train Epoch: 5 [64000/112800 (57%)]\tLoss: 0.927097\n",
            "Train Epoch: 5 [96000/112800 (85%)]\tLoss: 0.859342\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 13433/18800 (71%)\n",
            "\n",
            "Train Epoch: 6 [0/112800 (0%)]\tLoss: 0.882377\n",
            "Train Epoch: 6 [32000/112800 (28%)]\tLoss: 1.086497\n",
            "Train Epoch: 6 [64000/112800 (57%)]\tLoss: 0.899486\n",
            "Train Epoch: 6 [96000/112800 (85%)]\tLoss: 0.896518\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 13446/18800 (72%)\n",
            "\n",
            "Train Epoch: 7 [0/112800 (0%)]\tLoss: 1.089098\n",
            "Train Epoch: 7 [32000/112800 (28%)]\tLoss: 0.822493\n",
            "Train Epoch: 7 [64000/112800 (57%)]\tLoss: 0.791857\n",
            "Train Epoch: 7 [96000/112800 (85%)]\tLoss: 1.151577\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 13461/18800 (72%)\n",
            "\n",
            "Train Epoch: 8 [0/112800 (0%)]\tLoss: 1.010590\n",
            "Train Epoch: 8 [32000/112800 (28%)]\tLoss: 1.052333\n",
            "Train Epoch: 8 [64000/112800 (57%)]\tLoss: 1.081598\n",
            "Train Epoch: 8 [96000/112800 (85%)]\tLoss: 0.980203\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 13475/18800 (72%)\n",
            "\n",
            "Train Epoch: 9 [0/112800 (0%)]\tLoss: 0.821734\n",
            "Train Epoch: 9 [32000/112800 (28%)]\tLoss: 1.156714\n",
            "Train Epoch: 9 [64000/112800 (57%)]\tLoss: 0.870496\n",
            "Train Epoch: 9 [96000/112800 (85%)]\tLoss: 1.213968\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 13476/18800 (72%)\n",
            "\n",
            "Train Epoch: 10 [0/112800 (0%)]\tLoss: 1.061002\n",
            "Train Epoch: 10 [32000/112800 (28%)]\tLoss: 0.860641\n",
            "Train Epoch: 10 [64000/112800 (57%)]\tLoss: 0.955180\n",
            "Train Epoch: 10 [96000/112800 (85%)]\tLoss: 1.018961\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 13483/18800 (72%)\n",
            "\n",
            "Train Epoch: 11 [0/112800 (0%)]\tLoss: 1.146939\n",
            "Train Epoch: 11 [32000/112800 (28%)]\tLoss: 1.031884\n",
            "Train Epoch: 11 [64000/112800 (57%)]\tLoss: 1.266201\n",
            "Train Epoch: 11 [96000/112800 (85%)]\tLoss: 0.839173\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 13483/18800 (72%)\n",
            "\n",
            "Train Epoch: 12 [0/112800 (0%)]\tLoss: 1.364381\n",
            "Train Epoch: 12 [32000/112800 (28%)]\tLoss: 0.893953\n",
            "Train Epoch: 12 [64000/112800 (57%)]\tLoss: 1.021714\n",
            "Train Epoch: 12 [96000/112800 (85%)]\tLoss: 1.307974\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 13484/18800 (72%)\n",
            "\n",
            "Train Epoch: 13 [0/112800 (0%)]\tLoss: 1.048823\n",
            "Train Epoch: 13 [32000/112800 (28%)]\tLoss: 1.113586\n",
            "Train Epoch: 13 [64000/112800 (57%)]\tLoss: 1.013187\n",
            "Train Epoch: 13 [96000/112800 (85%)]\tLoss: 1.087553\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 13489/18800 (72%)\n",
            "\n",
            "Train Epoch: 14 [0/112800 (0%)]\tLoss: 0.883004\n",
            "Train Epoch: 14 [32000/112800 (28%)]\tLoss: 1.169466\n",
            "Train Epoch: 14 [64000/112800 (57%)]\tLoss: 1.009674\n",
            "Train Epoch: 14 [96000/112800 (85%)]\tLoss: 1.006685\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 13497/18800 (72%)\n",
            "\n",
            "Train Epoch: 15 [0/112800 (0%)]\tLoss: 0.844265\n",
            "Train Epoch: 15 [32000/112800 (28%)]\tLoss: 1.204563\n",
            "Train Epoch: 15 [64000/112800 (57%)]\tLoss: 0.947007\n",
            "Train Epoch: 15 [96000/112800 (85%)]\tLoss: 1.047104\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 13499/18800 (72%)\n",
            "\n",
            "Train Epoch: 16 [0/112800 (0%)]\tLoss: 0.824023\n",
            "Train Epoch: 16 [32000/112800 (28%)]\tLoss: 0.957251\n",
            "Train Epoch: 16 [64000/112800 (57%)]\tLoss: 0.927920\n",
            "Train Epoch: 16 [96000/112800 (85%)]\tLoss: 1.074985\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 13501/18800 (72%)\n",
            "\n",
            "Train Epoch: 17 [0/112800 (0%)]\tLoss: 0.725426\n",
            "Train Epoch: 17 [32000/112800 (28%)]\tLoss: 1.003995\n",
            "Train Epoch: 17 [64000/112800 (57%)]\tLoss: 0.893178\n",
            "Train Epoch: 17 [96000/112800 (85%)]\tLoss: 1.153580\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 13506/18800 (72%)\n",
            "\n",
            "Train Epoch: 18 [0/112800 (0%)]\tLoss: 1.238509\n",
            "Train Epoch: 18 [32000/112800 (28%)]\tLoss: 0.791025\n",
            "Train Epoch: 18 [64000/112800 (57%)]\tLoss: 0.843500\n",
            "Train Epoch: 18 [96000/112800 (85%)]\tLoss: 0.997475\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 13508/18800 (72%)\n",
            "\n",
            "Train Epoch: 19 [0/112800 (0%)]\tLoss: 1.084102\n",
            "Train Epoch: 19 [32000/112800 (28%)]\tLoss: 0.956041\n",
            "Train Epoch: 19 [64000/112800 (57%)]\tLoss: 0.729033\n",
            "Train Epoch: 19 [96000/112800 (85%)]\tLoss: 1.004393\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 13510/18800 (72%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zHE1hHnolZh",
        "colab_type": "text"
      },
      "source": [
        "Adam, lr (0.01) >> Accuracy of 88%\\\n",
        "Adagrad, lr (0.01) >> Accuracy of 73%\\\n",
        " ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK65414DxkXO",
        "colab_type": "text"
      },
      "source": [
        "#3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZtcS23Gxm9S",
        "colab_type": "text"
      },
      "source": [
        "Train the same classifier on MNIST dataset (you will have to replace the last dense layer of your model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXqk_Q_NyYaE",
        "colab_type": "text"
      },
      "source": [
        "*MNIST*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0zoy24ayWoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "train_dataset_mnist = datasets.MNIST(\n",
        "                                '/data', \n",
        "                                train = True, \n",
        "                                download = True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "test_dataset_mnist = datasets.MNIST(\n",
        "                                '../data', \n",
        "                                train = False, \n",
        "                                download = True,\n",
        "                                transform = transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "train_loader_mnist = torch.utils.data.DataLoader(\n",
        "                                        train_dataset_mnist, \n",
        "                                        batch_size=64, \n",
        "                                        shuffle=True)\n",
        "\n",
        "test_loader_mnist = torch.utils.data.DataLoader(\n",
        "                                        test_dataset_mnist, \n",
        "                                        batch_size=64, \n",
        "                                        shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owI1ERSwzb4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTNet(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super(MNISTNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10) # changed from self.fc2 = nn.Linear(1000, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model_mnist = MNISTNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCbe1-PLz_EN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3aa0cf5-1f00-4f90-e3c0-6278884decd9"
      },
      "source": [
        "model_mnist = model.to(device)\n",
        "\n",
        "optimizer_mnist_1 = torch.optim.Adam(\n",
        "                    model_mnist.parameters(), \n",
        "                    lr=learning_rate)  \n",
        "\n",
        "optimizer_mnist = torch.optim.Adagrad(\n",
        "                    model_mnist.parameters(),\n",
        "                    lr = learning_rate, \n",
        "                    lr_decay = 0.01)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "                                            optimizer_mnist,\n",
        "                                            0.5)\n",
        "\n",
        "writer_3_i = SummaryWriter(log_dir=f\"log_3i\")\n",
        "for epoch in range(20):\n",
        "    train_ass(writer_3_i, model_mnist, train_loader_mnist, optimizer_mnist, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(writer_3_i, model_mnist, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 6.844205\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.124841\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 9579/10000 (96%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.378308\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.160015\n",
            "\n",
            "Test set: Average loss: 0.0022, Accuracy: 9605/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.187679\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.218061\n",
            "\n",
            "Test set: Average loss: 0.0022, Accuracy: 9616/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.233760\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.099531\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9626/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.166952\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.239151\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9627/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.144863\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.165058\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9630/10000 (96%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.101108\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.093360\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9632/10000 (96%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.086467\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.157431\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9635/10000 (96%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.177830\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.086480\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9635/10000 (96%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.127186\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.178279\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9637/10000 (96%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.116907\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.122487\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9637/10000 (96%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.159404\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.354306\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9641/10000 (96%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.120773\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.107515\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9642/10000 (96%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.136499\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.200713\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9644/10000 (96%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.118189\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.072076\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9646/10000 (96%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.110822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRqnbUKV2hbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer_3_ii = SummaryWriter(log_dir=f\"log_3\")\n",
        "for epoch in range(20):\n",
        "    train_ass(writer_3_ii, model_mnist, train_loader_mnist, optimizer_mnist_1, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(writer_3_ii, model_mnist, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Dbuzvu0reQ",
        "colab_type": "text"
      },
      "source": [
        "Adagrad(96%)\\\n",
        "Adam(99%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdq0bMGc0yPQ",
        "colab_type": "text"
      },
      "source": [
        "#4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyc6MdSE4Y_s",
        "colab_type": "text"
      },
      "source": [
        "Use EMNIST classifier to classify MNIST test dataset.\\\n",
        "\n",
        "* To do this you'll need to restrict last layer to the labels which correspond to digits and choose the largest value \n",
        "\n",
        "(\\\n",
        "    torch tensors can be indexed with lists, like this:\\\n",
        "    --> torch.tensor(\n",
        "        [\n",
        "            [1,2],\n",
        "            [3,4], \n",
        "            [5, 6]\n",
        "        ])[[0, 1]]\\\n",
        ").\\\n",
        "\\\n",
        "Which of the models performs better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rg3alv4v6MjO",
        "colab": {}
      },
      "source": [
        "def test_ass_2(writer, model, test_loader, loss_function, device = device):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "    \n",
        "            output = model(data)\n",
        "    \n",
        "            test_loss += loss_function(output[:,0:10], target).sum().item()\n",
        "    \n",
        "            pred = output[:,0:10].argmax(dim=1, keepdim=True)\n",
        "    \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    writer.add_scalar(\"test_loss\", test_loss, global_step=epoch)\n",
        "    writer.add_scalar(\"accuracy\", accuracy, global_step=epoch)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IyIe3Tlx6Mjq",
        "colab": {}
      },
      "source": [
        "def train_ass_2(writer, model, train_loader, optimizer, loss_function, epoch , device = device):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = loss_function(output[:,0:10], target)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    writer.add_scalar(\"train_loss\", train_loss, global_step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwIdmP-J6lBA",
        "colab_type": "text"
      },
      "source": [
        "for epoch in range(20):\n",
        "    train_ass_2(model, train_loader_mnist, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass_2(model, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LeR-3ofJF2kb",
        "colab": {}
      },
      "source": [
        "writer_4_i = SummaryWriter(log_dir=f\"log4_i\")\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_ass_2(writer_4_i, model, train_loader_mnist, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass_2(writer_4_i, model, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uTkhg2_7j7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer_4_ii = SummaryWriter(log_dir=f\"log4_ii\")\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_ass_2(writer_4_ii, model, train_loader_mnist, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass_2(writer_4_ii, model, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnRaI-jsDkFn",
        "colab_type": "text"
      },
      "source": [
        "Adam(99%)\\\n",
        "Adagrad(99%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWswqCv1HTsl",
        "colab_type": "text"
      },
      "source": [
        "EMNIST is an extention of MNIST dataset.\n",
        "It has 47 classes (handwritten digits and leters), some of the letters are represented as two classes (upper and lower case) and some (ex. o, s) has only one class associated with them.\n",
        "\n",
        "1. Find correspondence between classess and letters/digits.\n",
        "2. Build and train a dense classifier for EMNIST dataset. Try to achieve the highest accuracy you can. Explain, what experiments have you perform. What optimizers have you tested? How network/optimizer parameters were chosen.\n",
        "3. Train the same classifier on MNIST dataset (you will have to replace the last dense layer of your model).\n",
        "4. Use EMNIST classifier to classify MNIST test dataset. To do this you'll need to restrict last layer to the labels which correspond to digits and choose the largest value (torch tensors can be indexed with lists, like this: `torch.tensor([[1,2],[3,4], [5, 6]])[[0, 1]]`). Which of the models performs better?\n",
        "5. Present a short (2-3 pages long) report on this experiment. What approaches have you tried? How can you explain the results you got? Include tensorboard plots in the experiment report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BylbtLYr0AVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}