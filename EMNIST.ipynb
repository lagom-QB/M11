{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lagom-QB/M11/blob/master/EMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UinBL84o1sml",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# EMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54606hJC0QEH",
        "colab_type": "text"
      },
      "source": [
        "EMNIST is an extention of MNIST dataset. It has 47 classes (handwritten digits and leters), some of the letters are represented as two classes (upper and lower case) and some (ex. o, s) has only one class associated with them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN6Uk4bp7bUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "train_dataset = datasets.EMNIST('/data', train=True, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]),\n",
        "                                split=\"balanced\")\n",
        "\n",
        "test_dataset = datasets.EMNIST('../data', train=False, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]),\n",
        "                                split=\"balanced\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXSDr7GIz_H1",
        "colab_type": "text"
      },
      "source": [
        "#1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzbIhZXw0ECa",
        "colab_type": "text"
      },
      "source": [
        "Find correspondence between classess and letters/digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLwX1Ah4OYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "letters = train_dataset.classes_split_dict['letters']\n",
        "digits  = train_dataset.classes_split_dict['digits']\n",
        "byclass = train_dataset.classes_split_dict['byclass']  # if not why not\n",
        "balanced = train_dataset.classes_split_dict['balanced']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXRQ_ta89Pv4",
        "colab_type": "code",
        "outputId": "79729c7e-1410-4879-e2dc-cf82f12c74a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(balanced)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10X16_Sz5gtP",
        "colab_type": "text"
      },
      "source": [
        "#2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE8iDFp05y0f",
        "colab_type": "text"
      },
      "source": [
        "Build and train a dense classifier for EMNIST dataset. Try to achieve the highest accuracy you can.\\\n",
        "Explain, what experiments have you perform.\\\n",
        "What optimizers have you tested?\\\n",
        "How network/optimizer parameters were chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpjCl-Ge6CXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.modules import loss\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTsGgOG28kaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001 # Small enough\n",
        "num_epochs = 25 #If not why not\n",
        "num_classes = len(balanced)\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdTfLAX8XNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "\n",
        "device    = torch.device(\"cuda\")\n",
        "# device    = torch.device(\"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_1 = torch.optim.Adam(\n",
        "                    model.parameters(), \n",
        "                    lr=learning_rate)  \n",
        "\n",
        "optimizer = torch.optim.Adagrad(\n",
        "                    model.parameters(),\n",
        "                    lr = learning_rate, \n",
        "                    lr_decay = 0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "                                            optimizer,\n",
        "                                            0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fey_rbnsYvMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_ass(writer, model, test_loader, loss_function, device = device):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "    \n",
        "            output = model(data)\n",
        "    \n",
        "            test_loss += loss_function(output, target).sum().item()\n",
        "    \n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "    \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    writer.add_scalar(\"test_loss\", test_loss, global_step=epoch)\n",
        "    writer.add_scalar(\"accuracy\", accuracy, global_step=epoch)\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbqw2Emv9aq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ass(writer, model, train_loader, optimizer, loss_function, epoch , device = device):\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    writer.add_scalar(\"train_loss\", train_loss, global_step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkm5eekaQliD",
        "colab_type": "code",
        "outputId": "f8e28740-f01e-4f4a-9b07-c6f4ff20229e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "writer_2_i = SummaryWriter(log_dir=f\"log_2i\")\n",
        "for epoch in range(20):\n",
        "    train_ass(writer_2_i, model, train_loader, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(writer_2_i, model, test_loader, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/112800 (0%)]\tLoss: 0.843950\n",
            "Train Epoch: 0 [32000/112800 (28%)]\tLoss: 1.076501\n",
            "Train Epoch: 0 [64000/112800 (57%)]\tLoss: 0.940363\n",
            "Train Epoch: 0 [96000/112800 (85%)]\tLoss: 1.121796\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 13512/18800 (72%)\n",
            "\n",
            "Train Epoch: 1 [0/112800 (0%)]\tLoss: 1.107953\n",
            "Train Epoch: 1 [32000/112800 (28%)]\tLoss: 0.964005\n",
            "Train Epoch: 1 [64000/112800 (57%)]\tLoss: 1.078275\n",
            "Train Epoch: 1 [96000/112800 (85%)]\tLoss: 0.859198\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 13547/18800 (72%)\n",
            "\n",
            "Train Epoch: 2 [0/112800 (0%)]\tLoss: 1.063351\n",
            "Train Epoch: 2 [32000/112800 (28%)]\tLoss: 1.262464\n",
            "Train Epoch: 2 [64000/112800 (57%)]\tLoss: 1.172318\n",
            "Train Epoch: 2 [96000/112800 (85%)]\tLoss: 1.200750\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 13560/18800 (72%)\n",
            "\n",
            "Train Epoch: 3 [0/112800 (0%)]\tLoss: 1.060959\n",
            "Train Epoch: 3 [32000/112800 (28%)]\tLoss: 0.994583\n",
            "Train Epoch: 3 [64000/112800 (57%)]\tLoss: 0.899548\n",
            "Train Epoch: 3 [96000/112800 (85%)]\tLoss: 1.003954\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 13574/18800 (72%)\n",
            "\n",
            "Train Epoch: 4 [0/112800 (0%)]\tLoss: 0.930944\n",
            "Train Epoch: 4 [32000/112800 (28%)]\tLoss: 0.838235\n",
            "Train Epoch: 4 [64000/112800 (57%)]\tLoss: 0.761022\n",
            "Train Epoch: 4 [96000/112800 (85%)]\tLoss: 0.978422\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 13581/18800 (72%)\n",
            "\n",
            "Train Epoch: 5 [0/112800 (0%)]\tLoss: 0.901258\n",
            "Train Epoch: 5 [32000/112800 (28%)]\tLoss: 0.872989\n",
            "Train Epoch: 5 [64000/112800 (57%)]\tLoss: 1.095061\n",
            "Train Epoch: 5 [96000/112800 (85%)]\tLoss: 0.619209\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 13590/18800 (72%)\n",
            "\n",
            "Train Epoch: 6 [0/112800 (0%)]\tLoss: 1.067324\n",
            "Train Epoch: 6 [32000/112800 (28%)]\tLoss: 1.222692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBnzMjBYIsDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer_2_ii = SummaryWriter(log_dir=f\"log_2ii\")\n",
        "for epoch in range(20):\n",
        "    train_ass(writer_2_ii, model, train_loader, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(writer_2_ii, model, test_loader, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zHE1hHnolZh",
        "colab_type": "text"
      },
      "source": [
        "Adam, lr (0.01) >> Accuracy of 72%\\\n",
        "Adagrad, lr (0.01) >> Accuracy of 73%\\\n",
        " ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK65414DxkXO",
        "colab_type": "text"
      },
      "source": [
        "#3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZtcS23Gxm9S",
        "colab_type": "text"
      },
      "source": [
        "Train the same classifier on MNIST dataset (you will have to replace the last dense layer of your model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXqk_Q_NyYaE",
        "colab_type": "text"
      },
      "source": [
        "*MNIST*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0zoy24ayWoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "train_dataset_mnist = datasets.MNIST(\n",
        "                                '/data', \n",
        "                                train = True, \n",
        "                                download = True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "test_dataset_mnist = datasets.MNIST(\n",
        "                                '../data', \n",
        "                                train = False, \n",
        "                                download = True,\n",
        "                                transform = transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "train_loader_mnist = torch.utils.data.DataLoader(\n",
        "                                        train_dataset_mnist, \n",
        "                                        batch_size=64, \n",
        "                                        shuffle=True)\n",
        "\n",
        "test_loader_mnist = torch.utils.data.DataLoader(\n",
        "                                        test_dataset_mnist, \n",
        "                                        batch_size=64, \n",
        "                                        shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owI1ERSwzb4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTNet(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super(MNISTNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10) # changed from self.fc2 = nn.Linear(1000, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model_mnist = MNISTNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCbe1-PLz_EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_mnist = model.to(device)\n",
        "\n",
        "optimizer_mnist_1 = torch.optim.Adam(\n",
        "                    model_mnist.parameters(), \n",
        "                    lr=learning_rate)  \n",
        "\n",
        "optimizer_mnist = torch.optim.Adagrad(\n",
        "                    model_mnist.parameters(),\n",
        "                    lr = learning_rate, \n",
        "                    lr_decay = 0.01)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "                                            optimizer_mnist,\n",
        "                                            0.5)\n",
        "\n",
        "writer_3_i = SummaryWriter(log_dir=f\"log_3i\")\n",
        "for epoch in range(20):\n",
        "    train_ass(writer_3_i, model_mnist, train_loader_mnist, optimizer_mnist, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(writer_3_i, model_mnist, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRqnbUKV2hbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer_3_ii = SummaryWriter(log_dir=f\"log_3ii\")\n",
        "for epoch in range(20):\n",
        "    train_ass(writer_3_ii, model_mnist, train_loader_mnist, optimizer_mnist_1, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(writer_3_ii, model_mnist, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Dbuzvu0reQ",
        "colab_type": "text"
      },
      "source": [
        "Adagrad(96%)\\\n",
        "Adam(99%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdq0bMGc0yPQ",
        "colab_type": "text"
      },
      "source": [
        "#4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyc6MdSE4Y_s",
        "colab_type": "text"
      },
      "source": [
        "Use EMNIST classifier to classify MNIST test dataset.\\\n",
        "\n",
        "* To do this you'll need to restrict last layer to the labels which correspond to digits and choose the largest value \n",
        "\n",
        "(\\\n",
        "    torch tensors can be indexed with lists, like this:\\\n",
        "    --> torch.tensor(\n",
        "        [\n",
        "            [1,2],\n",
        "            [3,4], \n",
        "            [5, 6]\n",
        "        ])[[0, 1]]\\\n",
        ").\\\n",
        "\\\n",
        "Which of the models performs better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rg3alv4v6MjO",
        "colab": {}
      },
      "source": [
        "def test_ass_2(writer, model, test_loader, loss_function, device = device):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "    \n",
        "            output = model(data)\n",
        "    \n",
        "            test_loss += loss_function(output[:,0:10], target).sum().item()\n",
        "    \n",
        "            pred = output[:,0:10].argmax(dim=1, keepdim=True)\n",
        "    \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    writer.add_scalar(\"test_loss\", test_loss, global_step=epoch)\n",
        "    writer.add_scalar(\"accuracy\", accuracy, global_step=epoch)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IyIe3Tlx6Mjq",
        "colab": {}
      },
      "source": [
        "def train_ass_2(writer, model, train_loader, optimizer, loss_function, epoch , device = device):\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = loss_function(output[:,0:10], target)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    writer.add_scalar(\"train_loss\", train_loss, global_step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwIdmP-J6lBA",
        "colab_type": "text"
      },
      "source": [
        "for epoch in range(20):\n",
        "    train_ass_2(model, train_loader_mnist, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass_2(model, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LeR-3ofJF2kb",
        "colab": {}
      },
      "source": [
        "writer_4_i = SummaryWriter(log_dir=f\"log_4i\")\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_ass_2(writer_4_i, model, train_loader_mnist, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass_2(writer_4_i, model, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uTkhg2_7j7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer_4_ii = SummaryWriter(log_dir=f\"log_4ii\")\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_ass_2(writer_4_ii, model, train_loader_mnist, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass_2(writer_4_ii, model, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnRaI-jsDkFn",
        "colab_type": "text"
      },
      "source": [
        "Adam(99%)\\\n",
        "Adagrad(99%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJSAKhZpIdRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir log_2i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEu2lJDGJBAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir log_2ii"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br7gP0KlJCNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir log_3i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDSJMVGPJDcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir log_3ii"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSg_GIyQJFDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir log_4i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egL4HiQvJGoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir log_4ii"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWswqCv1HTsl",
        "colab_type": "text"
      },
      "source": [
        "EMNIST is an extention of MNIST dataset.\n",
        "It has 47 classes (handwritten digits and leters), some of the letters are represented as two classes (upper and lower case) and some (ex. o, s) has only one class associated with them.\n",
        "\n",
        "1. Find correspondence between classess and letters/digits.\n",
        "2. Build and train a dense classifier for EMNIST dataset. Try to achieve the highest accuracy you can. Explain, what experiments have you perform. What optimizers have you tested? How network/optimizer parameters were chosen.\n",
        "3. Train the same classifier on MNIST dataset (you will have to replace the last dense layer of your model).\n",
        "4. Use EMNIST classifier to classify MNIST test dataset. To do this you'll need to restrict last layer to the labels which correspond to digits and choose the largest value (torch tensors can be indexed with lists, like this: `torch.tensor([[1,2],[3,4], [5, 6]])[[0, 1]]`). Which of the models performs better?\n",
        "5. Present a short (2-3 pages long) report on this experiment. What approaches have you tried? How can you explain the results you got? Include tensorboard plots in the experiment report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BylbtLYr0AVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}