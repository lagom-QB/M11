{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lagom-QB/M11/blob/master/EMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UinBL84o1sml",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# EMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54606hJC0QEH",
        "colab_type": "text"
      },
      "source": [
        "EMNIST is an extention of MNIST dataset. It has 47 classes (handwritten digits and leters), some of the letters are represented as two classes (upper and lower case) and some (ex. o, s) has only one class associated with them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN6Uk4bp7bUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "train_dataset = datasets.EMNIST('/data', train=True, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]),\n",
        "                                split=\"balanced\")\n",
        "\n",
        "test_dataset = datasets.EMNIST('../data', train=False, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]),\n",
        "                                split=\"balanced\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXSDr7GIz_H1",
        "colab_type": "text"
      },
      "source": [
        "#1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzbIhZXw0ECa",
        "colab_type": "text"
      },
      "source": [
        "Find correspondence between classess and letters/digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLwX1Ah4OYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "letters = train_dataset.classes_split_dict['letters']\n",
        "digits  = train_dataset.classes_split_dict['digits']\n",
        "byclass = train_dataset.classes_split_dict['byclass']  # if not why not\n",
        "balanced = train_dataset.classes_split_dict['balanced']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXRQ_ta89Pv4",
        "colab_type": "code",
        "outputId": "97d759ba-93a5-4988-f31e-73f4c01d38a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "len(balanced)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10X16_Sz5gtP",
        "colab_type": "text"
      },
      "source": [
        "#2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE8iDFp05y0f",
        "colab_type": "text"
      },
      "source": [
        "Build and train a dense classifier for EMNIST dataset. Try to achieve the highest accuracy you can.\\\n",
        "Explain, what experiments have you perform.\\\n",
        "What optimizers have you tested?\\\n",
        "How network/optimizer parameters were chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpjCl-Ge6CXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.modules import loss\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTsGgOG28kaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001 # Small enough\n",
        "num_epochs = 25 #If not why not\n",
        "num_classes = len(balanced)\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdTfLAX8XNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "\n",
        "device    = torch.device(\"cuda\")\n",
        "# device    = torch.device(\"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_1 = torch.optim.Adam(\n",
        "                    model.parameters(), \n",
        "                    lr=learning_rate)  \n",
        "\n",
        "optimizer = torch.optim.Adagrad(\n",
        "                    model.parameters(),\n",
        "                    lr = learning_rate, \n",
        "                    lr_decay = 0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "                                            optimizer,\n",
        "                                            0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fey_rbnsYvMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_ass(model, test_loader, loss_function, device = device):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "    \n",
        "            output = model(data)\n",
        "    \n",
        "            test_loss += loss_function(output, target).sum().item()\n",
        "    \n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "    \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbqw2Emv9aq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ass(model, train_loader, optimizer, loss_function, epoch , device = device):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkm5eekaQliD",
        "colab_type": "code",
        "outputId": "5449b671-e97b-48e3-c939-ebaa9de7822a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(20):\n",
        "    train_ass(model, train_loader, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(model, test_loader, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/112800 (0%)]\tLoss: 4.155716\n",
            "Train Epoch: 0 [32000/112800 (28%)]\tLoss: 1.055035\n",
            "Train Epoch: 0 [64000/112800 (57%)]\tLoss: 0.890459\n",
            "Train Epoch: 0 [96000/112800 (85%)]\tLoss: 0.776751\n",
            "\n",
            "Test set: Average loss: 0.0155, Accuracy: 13457/18800 (72%)\n",
            "\n",
            "Train Epoch: 1 [0/112800 (0%)]\tLoss: 1.485131\n",
            "Train Epoch: 1 [32000/112800 (28%)]\tLoss: 1.204287\n",
            "Train Epoch: 1 [64000/112800 (57%)]\tLoss: 0.910169\n",
            "Train Epoch: 1 [96000/112800 (85%)]\tLoss: 0.917098\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 13561/18800 (72%)\n",
            "\n",
            "Train Epoch: 2 [0/112800 (0%)]\tLoss: 1.197139\n",
            "Train Epoch: 2 [32000/112800 (28%)]\tLoss: 1.153221\n",
            "Train Epoch: 2 [64000/112800 (57%)]\tLoss: 1.021487\n",
            "Train Epoch: 2 [96000/112800 (85%)]\tLoss: 0.844010\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 13601/18800 (72%)\n",
            "\n",
            "Train Epoch: 3 [0/112800 (0%)]\tLoss: 1.303065\n",
            "Train Epoch: 3 [32000/112800 (28%)]\tLoss: 0.996297\n",
            "Train Epoch: 3 [64000/112800 (57%)]\tLoss: 1.142824\n",
            "Train Epoch: 3 [96000/112800 (85%)]\tLoss: 1.071908\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 13644/18800 (73%)\n",
            "\n",
            "Train Epoch: 4 [0/112800 (0%)]\tLoss: 1.072531\n",
            "Train Epoch: 4 [32000/112800 (28%)]\tLoss: 1.136519\n",
            "Train Epoch: 4 [64000/112800 (57%)]\tLoss: 1.082862\n",
            "Train Epoch: 4 [96000/112800 (85%)]\tLoss: 1.262197\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 13644/18800 (73%)\n",
            "\n",
            "Train Epoch: 5 [0/112800 (0%)]\tLoss: 0.762451\n",
            "Train Epoch: 5 [32000/112800 (28%)]\tLoss: 1.005683\n",
            "Train Epoch: 5 [64000/112800 (57%)]\tLoss: 0.774664\n",
            "Train Epoch: 5 [96000/112800 (85%)]\tLoss: 0.836624\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 13659/18800 (73%)\n",
            "\n",
            "Train Epoch: 6 [0/112800 (0%)]\tLoss: 1.142190\n",
            "Train Epoch: 6 [32000/112800 (28%)]\tLoss: 0.966437\n",
            "Train Epoch: 6 [64000/112800 (57%)]\tLoss: 0.972488\n",
            "Train Epoch: 6 [96000/112800 (85%)]\tLoss: 1.104957\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 13669/18800 (73%)\n",
            "\n",
            "Train Epoch: 7 [0/112800 (0%)]\tLoss: 0.998271\n",
            "Train Epoch: 7 [32000/112800 (28%)]\tLoss: 0.966555\n",
            "Train Epoch: 7 [64000/112800 (57%)]\tLoss: 0.742148\n",
            "Train Epoch: 7 [96000/112800 (85%)]\tLoss: 0.905121\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 13673/18800 (73%)\n",
            "\n",
            "Train Epoch: 8 [0/112800 (0%)]\tLoss: 0.880547\n",
            "Train Epoch: 8 [32000/112800 (28%)]\tLoss: 1.006859\n",
            "Train Epoch: 8 [64000/112800 (57%)]\tLoss: 0.919715\n",
            "Train Epoch: 8 [96000/112800 (85%)]\tLoss: 1.157781\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 13672/18800 (73%)\n",
            "\n",
            "Train Epoch: 9 [0/112800 (0%)]\tLoss: 1.015220\n",
            "Train Epoch: 9 [32000/112800 (28%)]\tLoss: 0.927026\n",
            "Train Epoch: 9 [64000/112800 (57%)]\tLoss: 1.139490\n",
            "Train Epoch: 9 [96000/112800 (85%)]\tLoss: 1.134926\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 13674/18800 (73%)\n",
            "\n",
            "Train Epoch: 10 [0/112800 (0%)]\tLoss: 0.802108\n",
            "Train Epoch: 10 [32000/112800 (28%)]\tLoss: 1.120425\n",
            "Train Epoch: 10 [64000/112800 (57%)]\tLoss: 0.909422\n",
            "Train Epoch: 10 [96000/112800 (85%)]\tLoss: 0.935962\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 13676/18800 (73%)\n",
            "\n",
            "Train Epoch: 11 [0/112800 (0%)]\tLoss: 1.091564\n",
            "Train Epoch: 11 [32000/112800 (28%)]\tLoss: 0.810464\n",
            "Train Epoch: 11 [64000/112800 (57%)]\tLoss: 1.210546\n",
            "Train Epoch: 11 [96000/112800 (85%)]\tLoss: 0.968327\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 13685/18800 (73%)\n",
            "\n",
            "Train Epoch: 12 [0/112800 (0%)]\tLoss: 1.258995\n",
            "Train Epoch: 12 [32000/112800 (28%)]\tLoss: 1.072209\n",
            "Train Epoch: 12 [64000/112800 (57%)]\tLoss: 1.103817\n",
            "Train Epoch: 12 [96000/112800 (85%)]\tLoss: 0.873524\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 13687/18800 (73%)\n",
            "\n",
            "Train Epoch: 13 [0/112800 (0%)]\tLoss: 0.800600\n",
            "Train Epoch: 13 [32000/112800 (28%)]\tLoss: 1.081765\n",
            "Train Epoch: 13 [64000/112800 (57%)]\tLoss: 0.683303\n",
            "Train Epoch: 13 [96000/112800 (85%)]\tLoss: 1.317804\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 13692/18800 (73%)\n",
            "\n",
            "Train Epoch: 14 [0/112800 (0%)]\tLoss: 0.938274\n",
            "Train Epoch: 14 [32000/112800 (28%)]\tLoss: 1.222979\n",
            "Train Epoch: 14 [64000/112800 (57%)]\tLoss: 1.038600\n",
            "Train Epoch: 14 [96000/112800 (85%)]\tLoss: 0.976125\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 13693/18800 (73%)\n",
            "\n",
            "Train Epoch: 15 [0/112800 (0%)]\tLoss: 1.108488\n",
            "Train Epoch: 15 [32000/112800 (28%)]\tLoss: 0.967291\n",
            "Train Epoch: 15 [64000/112800 (57%)]\tLoss: 0.774887\n",
            "Train Epoch: 15 [96000/112800 (85%)]\tLoss: 0.823890\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 13694/18800 (73%)\n",
            "\n",
            "Train Epoch: 16 [0/112800 (0%)]\tLoss: 0.895965\n",
            "Train Epoch: 16 [32000/112800 (28%)]\tLoss: 0.846084\n",
            "Train Epoch: 16 [64000/112800 (57%)]\tLoss: 1.015553\n",
            "Train Epoch: 16 [96000/112800 (85%)]\tLoss: 0.952820\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 13693/18800 (73%)\n",
            "\n",
            "Train Epoch: 17 [0/112800 (0%)]\tLoss: 0.909039\n",
            "Train Epoch: 17 [32000/112800 (28%)]\tLoss: 0.918114\n",
            "Train Epoch: 17 [64000/112800 (57%)]\tLoss: 0.945867\n",
            "Train Epoch: 17 [96000/112800 (85%)]\tLoss: 0.856877\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 13696/18800 (73%)\n",
            "\n",
            "Train Epoch: 18 [0/112800 (0%)]\tLoss: 0.782256\n",
            "Train Epoch: 18 [32000/112800 (28%)]\tLoss: 1.128686\n",
            "Train Epoch: 18 [64000/112800 (57%)]\tLoss: 1.086310\n",
            "Train Epoch: 18 [96000/112800 (85%)]\tLoss: 0.947811\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 13697/18800 (73%)\n",
            "\n",
            "Train Epoch: 19 [0/112800 (0%)]\tLoss: 1.403769\n",
            "Train Epoch: 19 [32000/112800 (28%)]\tLoss: 0.791671\n",
            "Train Epoch: 19 [64000/112800 (57%)]\tLoss: 1.022872\n",
            "Train Epoch: 19 [96000/112800 (85%)]\tLoss: 0.728821\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 13700/18800 (73%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zHE1hHnolZh",
        "colab_type": "text"
      },
      "source": [
        "Adam, lr (0.01) >> Accuracy of 88%\\\n",
        "Adagrad, lr (0.01) >> Accuracy of 73%\\\n",
        " ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK65414DxkXO",
        "colab_type": "text"
      },
      "source": [
        "#3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZtcS23Gxm9S",
        "colab_type": "text"
      },
      "source": [
        "Train the same classifier on MNIST dataset (you will have to replace the last dense layer of your model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXqk_Q_NyYaE",
        "colab_type": "text"
      },
      "source": [
        "*MNIST*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0zoy24ayWoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "train_dataset_mnist = datasets.MNIST(\n",
        "                                '/data', \n",
        "                                train = True, \n",
        "                                download = True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "test_dataset_mnist = datasets.MNIST(\n",
        "                                '../data', \n",
        "                                train = False, \n",
        "                                download = True,\n",
        "                                transform = transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "train_loader_mnist = torch.utils.data.DataLoader(\n",
        "                                        train_dataset_mnist, \n",
        "                                        batch_size=64, \n",
        "                                        shuffle=True)\n",
        "\n",
        "test_loader_mnist = torch.utils.data.DataLoader(\n",
        "                                        test_dataset_mnist, \n",
        "                                        batch_size=64, \n",
        "                                        shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owI1ERSwzb4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTNet(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super(MNISTNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10) # changed from self.fc2 = nn.Linear(1000, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model_mnist = MNISTNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCbe1-PLz_EN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58e2dcf5-27b9-4e17-95ef-694a29498ecf"
      },
      "source": [
        "model_mnist = model.to(device)\n",
        "\n",
        "optimizer_mnist_1 = torch.optim.Adam(\n",
        "                    model_mnist.parameters(), \n",
        "                    lr=learning_rate)  \n",
        "\n",
        "optimizer_mnist = torch.optim.Adagrad(\n",
        "                    model_mnist.parameters(),\n",
        "                    lr = learning_rate, \n",
        "                    lr_decay = 0.01)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "                                            optimizer_mnist,\n",
        "                                            0.5)\n",
        "for epoch in range(20):\n",
        "    train_ass(model_mnist, train_loader_mnist, optimizer_mnist, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(model_mnist, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 7.591644\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.249015\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 9581/10000 (96%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.081704\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.175206\n",
            "\n",
            "Test set: Average loss: 0.0022, Accuracy: 9603/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.303017\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.325987\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9612/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.289603\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.102545\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9618/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.309292\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.269517\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9622/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.149467\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.112939\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9624/10000 (96%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.195254\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.264340\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9623/10000 (96%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.180816\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.056367\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9625/10000 (96%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.075521\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.113528\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9625/10000 (96%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.116415\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.220747\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9625/10000 (96%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.086985\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.283355\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9629/10000 (96%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.177028\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.095897\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9628/10000 (96%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.107289\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.172215\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9630/10000 (96%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.224085\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.223702\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9631/10000 (96%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.212635\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.069726\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9632/10000 (96%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.093597\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.061334\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9633/10000 (96%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.148002\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.113957\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9635/10000 (96%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.136260\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.136271\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9636/10000 (96%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.086227\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.124167\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9635/10000 (96%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.153293\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.163898\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9637/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRqnbUKV2hbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "105b0dce-8dd3-45a6-adc3-c52e4065e2e5"
      },
      "source": [
        "for epoch in range(20):\n",
        "    train_ass(model_mnist, train_loader_mnist, optimizer_mnist_1, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass(model_mnist, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 12.542337\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.042926\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 9866/10000 (99%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.035255\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.082642\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.014404\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.022710\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 9881/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.007643\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.017210\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9902/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.067244\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.184245\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9899/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.020625\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.070794\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.082387\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.032355\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9899/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.001020\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.085079\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9898/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.017319\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.005069\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.014210\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.017909\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 9902/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000630\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000654\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 9899/10000 (99%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.006415\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.143339\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.015064\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.159497\n",
            "\n",
            "Test set: Average loss: 0.0007, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.000748\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.019740\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.001299\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.013174\n",
            "\n",
            "Test set: Average loss: 0.0009, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.237366\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.213433\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9917/10000 (99%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.037109\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.117129\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 9915/10000 (99%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000380\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.024987\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.000412\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.002684\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9919/10000 (99%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.020532\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.014017\n",
            "\n",
            "Test set: Average loss: 0.0007, Accuracy: 9892/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Dbuzvu0reQ",
        "colab_type": "text"
      },
      "source": [
        "Adagrad(96%)\\\n",
        "Adam(99%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdq0bMGc0yPQ",
        "colab_type": "text"
      },
      "source": [
        "#4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyc6MdSE4Y_s",
        "colab_type": "text"
      },
      "source": [
        "Use EMNIST classifier to classify MNIST test dataset.\\\n",
        "\n",
        "* To do this you'll need to restrict last layer to the labels which correspond to digits and choose the largest value \n",
        "\n",
        "(\\\n",
        "    torch tensors can be indexed with lists, like this:\\\n",
        "    --> torch.tensor(\n",
        "        [\n",
        "            [1,2],\n",
        "            [3,4], \n",
        "            [5, 6]\n",
        "        ])[[0, 1]]\\\n",
        ").\\\n",
        "\\\n",
        "Which of the models performs better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rg3alv4v6MjO",
        "colab": {}
      },
      "source": [
        "def test_ass_2(model, test_loader, loss_function, device = device):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "    \n",
        "            output = model(data)\n",
        "    \n",
        "            test_loss += loss_function(output[:,0:10], target).sum().item()\n",
        "    \n",
        "            pred = output[:,0:10].argmax(dim=1, keepdim=True)\n",
        "    \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IyIe3Tlx6Mjq",
        "colab": {}
      },
      "source": [
        "def train_ass_2(model, train_loader, optimizer, loss_function, epoch , device = device):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = loss_function(output[:,0:10], target)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwIdmP-J6lBA",
        "colab_type": "text"
      },
      "source": [
        "for epoch in range(20):\n",
        "    train_ass_2(model, train_loader_mnist, optimizer, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass_2(model, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uTkhg2_7j7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "f52e03ac-e460-4b05-b4d7-b780ee78fa43"
      },
      "source": [
        "for epoch in range(20):\n",
        "    train_ass_2(model, train_loader_mnist, optimizer_1, loss.CrossEntropyLoss(), epoch)    \n",
        "    test_ass_2(model, test_loader_mnist, loss.CrossEntropyLoss(), device = device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.008963\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.013973\n",
            "\n",
            "Test set: Average loss: 0.0008, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.127903\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.000499\n",
            "\n",
            "Test set: Average loss: 0.0008, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.032082\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.002516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e7da963b2cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_ass_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_ass_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-c6cd4da9d595>\u001b[0m in \u001b[0;36mtrain_ass_2\u001b[0;34m(model, train_loader, optimizer, loss_function, epoch, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnRaI-jsDkFn",
        "colab_type": "text"
      },
      "source": [
        "Adam(99%)\\\n",
        "Adagrad(99%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWswqCv1HTsl",
        "colab_type": "text"
      },
      "source": [
        "EMNIST is an extention of MNIST dataset.\n",
        "It has 47 classes (handwritten digits and leters), some of the letters are represented as two classes (upper and lower case) and some (ex. o, s) has only one class associated with them.\n",
        "\n",
        "1. Find correspondence between classess and letters/digits.\n",
        "2. Build and train a dense classifier for EMNIST dataset. Try to achieve the highest accuracy you can. Explain, what experiments have you perform. What optimizers have you tested? How network/optimizer parameters were chosen.\n",
        "3. Train the same classifier on MNIST dataset (you will have to replace the last dense layer of your model).\n",
        "4. Use EMNIST classifier to classify MNIST test dataset. To do this you'll need to restrict last layer to the labels which correspond to digits and choose the largest value (torch tensors can be indexed with lists, like this: `torch.tensor([[1,2],[3,4], [5, 6]])[[0, 1]]`). Which of the models performs better?\n",
        "5. Present a short (2-3 pages long) report on this experiment. What approaches have you tried? How can you explain the results you got? Include tensorboard plots in the experiment report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BylbtLYr0AVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}