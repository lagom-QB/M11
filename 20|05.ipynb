{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNyvfYx5xuhZsXq6GySvaPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lagom-QB/M11/blob/master/20%7C05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6BGJqWeiKGQ",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------\n",
        "Update training and test function in such a way that train loss is ploted on \"train_loss\" graph, test loss on \"test_loss\" graph and accuracy on the test set on \"test_accuracy\" graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xLbOh8xh6zy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "9450c3b9-dda4-4f86-cc7f-749e797b71f6"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn.modules import loss\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "train_dataset = datasets.MNIST('/data', train=True, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "test_dataset = datasets.MNIST('../data', train=False, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "class MySGD(torch.optim.Optimizer):  \n",
        "  def __init__(self, params, lr):\n",
        "    self._lr = lr\n",
        "    defaults = {}\n",
        "    super().__init__(params, defaults)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self):\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        d_p = p.grad\n",
        "        p.add_(d_p, alpha=-self._lr)\n",
        "    return loss\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    return x.view(batch_size, -1)\n",
        "\n",
        "def get_model():\n",
        "  return nn.Sequential(Flatten(), \n",
        "                       nn.Linear(784, 512), \n",
        "                       nn.Tanh(),\n",
        "                       nn.Linear(512, 64), \n",
        "                       nn.Tanh(),\n",
        "                       nn.Linear(64, 10))\n",
        "\n",
        "def train_writer(writer, model, train_loader, optimizer, loss_function, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target)\n",
        "        loss.backward()\n",
        "        writer.add_scalar(\"train_loss\", torch.tensor(loss.item()), global_step = epoch)\n",
        "        optimizer.step()\n",
        "        if batch_idx % 200 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test_writer(writer, model, test_loader, loss_function):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += loss_function(output, target).sum().item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            writer.add_scalar(\"test_accuracy\", torch.tensor(correct), global_step = epoch)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    writer.add_scalar(\"test_loss\", torch.tensor(test_loss), global_step = epoch)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "loss_function = loss.CrossEntropyLoss()\n",
        "writer = SummaryWriter(log_dir=\"ass_1_logs\")\n",
        "model = get_model()\n",
        "optim = MySGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(1, 5):\n",
        "  train_writer(writer, model, train_loader, optim, loss_function, epoch)\n",
        "  test_writer(writer, model, test_loader, loss_function)\n",
        "writer.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.324755\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.472108\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.423116\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.264435\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.343311\n",
            "\n",
            "Test set: Average loss: 0.0031, Accuracy: 9425/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.291894\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.112025\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.113538\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.143097\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.085854\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9608/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.183233\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.038941\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.073978\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.013608\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.132436\n",
            "\n",
            "Test set: Average loss: 0.0014, Accuracy: 9728/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.081560\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.062478\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.069629\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.058251\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.049404\n",
            "\n",
            "Test set: Average loss: 0.0013, Accuracy: 9742/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1jLx0kpibn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "b2b0b49f-00b0-4bd3-8c66-9b3e99efd303"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ass_1_logs"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuiwzndLj8y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}